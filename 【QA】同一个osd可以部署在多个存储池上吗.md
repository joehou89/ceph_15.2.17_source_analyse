# 问题描述：  
ceph分布式存储系统中，同一个osd可以部署在多个存储池上吗？为什么    
这是一个群里有一位朋友问到的问题，他的原问题是ceph两个池，不复用osd，一个池满了不可写会导致另一个不满的池不能写吗？    
答案是不会  
  
# 问题分析:    
那么同一个osd可以部署到两个池上吗    
答案是可以，之前一直没有这么干过，所以潜意识中认为不能，早先的理解是osd--> 唯一创建对应的存储池pool，pool上pg唯一，即pg和osd是1对多的绝对关系，而不允许是同一个pg映射到多个pool的osd上。  
这个理解是混乱的，说明对ceph数据分布的原理还不清晰   
  
## 1. osd和创建的存储池没有直接的关系，我们之前的基于ceph做自研全闪产品，osd是属于硬盘池的东西，在这之上创建存储池，存储池的创建要根据crushmap的拓扑结构来设计，比如同一个osd是否会跨不同的存储池  
正常来说，比如要建立一个块存储副本池，单节点有两块osd，其拓扑结构如下，这是不包含机架的角色：    
![](https://github.com/joehou89/ceph_15.2.17_sourcecode_analyse/blob/main/crushmap%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84.png)    
这里的都是物理资源，crushmap里针对每一个pool，还有一个rule规则，其主要内容如下：  
```sh
id   #序号
type #表示副本类型还是ec类型
step take root  #root类型
...
```    
如果创建pool的rule规则，buckets所属的root是相同的，则同一个osd会部署上不同的存储池上，如果root是不同的，则每一个pool的osd是不同的，即物理隔离；    
我理解ceph之所以允许这样做的目的还是存储系统的通用化，即复用存储硬件资源，提供更多的存储服务，随之带来的问题是运维成本的增加，以及存储服务的性能是否满足业务需求等问题；  
正常来说，还是要保证每一个osd属于唯一的pool，确保物理隔离，这也是常规用法。    
  
## 2.如果同一个osd部署到两个存储池pool上，那么可能存在如下pg到osd的关系：  
```sh  
pool1 80.0e    pg  {11,2,5}
pool2 81.0e    pg  {2,5,11}
```  
可以看到同一个osd.11上包含两个pool池的pg，而不是唯一的，QA 哪里来保存不同pool的pgmap？这张pgmap在osd实例上是否也会看到？    
    
## 3.ceph创池流程  










